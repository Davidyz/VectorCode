*VectorCode.txt*                For                 Last change: 2025 March 10

==============================================================================
Table of Contents                               *VectorCode-table-of-contents*

1. NeoVim Plugin                                    |VectorCode-neovim-plugin|
  - Installation                       |VectorCode-neovim-plugin-installation|
  - Quick Start                         |VectorCode-neovim-plugin-quick-start|
  - User Command                       |VectorCode-neovim-plugin-user-command|
  - Configuration                     |VectorCode-neovim-plugin-configuration|
  - API Usage                             |VectorCode-neovim-plugin-api-usage|
  - Integrations                       |VectorCode-neovim-plugin-integrations|

==============================================================================
1. NeoVim Plugin                                    *VectorCode-neovim-plugin*


  [!NOTE] This plugin depends on the CLI tool. Please go through the CLI
  documentation <./cli.md> and make sure the VectorCode CLI is working before
  proceeding.

  [!NOTE] When the neovim plugin doesn’t work properly, please try upgrading
  the CLI tool to the latest version before opening an issue.
- |VectorCode-installation|
- |VectorCode-quick-start|
- |VectorCode-user-command|
    - |VectorCode-`vectorcode-register`|
    - |VectorCode-`vectorcode-deregister`|
- |VectorCode-configuration|
    - |VectorCode-`setup(opts?)`|
- |VectorCode-api-usage|
    - |VectorCode-synchronous-api|
        - |VectorCode-`query(query_message,-opts?,-callback?)`|
        - |VectorCode-`check(check_item?)`|
        - |VectorCode-`update(project_root?)`|
    - |VectorCode-cached-asynchronous-api|
        - |VectorCode-`cacher_backend.register_buffer(bufnr?,-opts?)`|
        - |VectorCode-`cacher_backend.query_from_cache(bufnr?)`|
        - |VectorCode-`cacher_backend.async_check(check_item?,-on_success?,-on_failure?)`|
        - |VectorCode-`cacher_backend.buf_is_registered(bufnr?)`|
        - |VectorCode-`cacher_backend.buf_is_enabled(bufnr?)`|
        - |VectorCode-`cacher_backend.buf_job_count(bufnr?)`|
        - |VectorCode-`cacher_backend.make_prompt_component(bufnr?,-component_cb?)`|
- |VectorCode-integrations|


INSTALLATION                           *VectorCode-neovim-plugin-installation*

Use your favourite plugin manager.

>lua
    {
      "Davidyz/VectorCode",
      version = "*", -- optional, depending on whether you're on nightly or release
      dependencies = { "nvim-lua/plenary.nvim" },
      cmd = "VectorCode", -- if you're lazy-loading VectorCode
    }
<

The VectorCode CLI and neovim plugin share the same release scheme (version
numbers). In other words, CLI 0.1.3 is guaranteed to work with neovim plugin
0.1.3, but if you use CLI 0.1.0 with neovim plugin 0.1.3, they may not work
together because the neovim plugin is built for a newer CLI release.

To ensure maximum compatibility, please either: 1. Use release build for
VectorCode CLI and pin to the release tags for the neovim plugin;

**OR**

1. Use the latest commit for the neovim plugin with VectorCode installed from
the latest GitHub commit.

It may be helpful to use a `build` hook to automatically upgrade the CLI when
the neovim plugin updates. For example, if you’re using lazy.nvim and `pipx`,
you can use the following plugin spec:

>lua
    {
      "Davidyz/VectorCode",
      version = "*", -- optional, depending on whether you're on nightly or release
      build = "pipx upgrade vectorcode", -- optional but recommended if you set `version = "*"`
      dependencies = { "nvim-lua/plenary.nvim" },
    }
<


QUICK START                             *VectorCode-neovim-plugin-quick-start*

For VectorCode to work with a LLM plugin, you need to somehow integrate the
query results into the prompt.

Here’s how VectorCode may be used with minuet-ai.nvim
<https://github.com/milanglacier/minuet-ai.nvim>:

>lua
    {
      "milanglacier/minuet-ai.nvim",
      config = function()
        -- This uses the async cache to accelerate the prompt construction.
        -- There's also the require('vectorcode').query API, which provides 
        -- more up-to-date information, but at the cost of blocking the main UI.
        local vectorcode_cacher = require("vectorcode.config").get_cacher_backend()
        require("minuet").setup({
          add_single_line_entry = true,
          n_completions = 1,
          -- I recommend you start with a small context window firstly, and gradually
          -- increase it based on your local computing power.
          context_window = 4096,
          after_cursor_filter_length = 30,
          notify = "debug",
          provider = "openai_fim_compatible",
          provider_options = {
            openai_fim_compatible = {
              api_key = "TERM",
              name = "Ollama",
              stream = true,
              end_point = "http://127.0.0.1:11434/v1/completions",
              model = "qwen2.5-coder:7b-base-q4_1",
              template = {
                prompt = function(pref, suff)
                  local prompt_message = ""
                  for _, file in ipairs(vectorcode_cacher.query_from_cache(0)) do
                    prompt_message = prompt_message .. "<|file_sep|>" .. file.path .. "\n" .. file.document
                  end
                  return prompt_message
                    .. "<|fim_prefix|>"
                    .. pref
                    .. "<|fim_suffix|>"
                    .. suff
                    .. "<|fim_middle|>"
                end,
                suffix = false,
              },
            },
          },
        })
      end,
    }
<


  [!NOTE] It can be challenging to find the best way to incorporate the
  project-level context into the prompt. The template above works well for
  `Qwen2.5-Coder` <https://github.com/QwenLM/Qwen2.5-Coder>, but may not work as
  intended for your LLM. I compiled the prompt construction code snippets for
  some other LLMs in the wiki
  <https://github.com/Davidyz/VectorCode/wiki/Prompt-Gallery>. Please check it
  out, because if your model was trained with project-level context support,
  you’ll have to modify the prompt structure accordingly to maximise its
  potential.
To use |VectorCode-async-cache|, you need to **register the buffer**. You may
either manually register a buffer using the |VectorCode-user-command|
`:VectorCode register`, or set up an autocommand:

>lua
    vim.api.nvim_create_autocmd("LspAttach", {
      callback = function()
        local bufnr = vim.api.nvim_get_current_buf()
        cacher.async_check("config", function()
          cacher.register_buffer(
            bufnr,
            { 
              n_query = 10,
            }
          )
        end, nil)
      end,
      desc = "Register buffer for VectorCode",
    })
<

to automatically register a new buffer. The autocommand is more convenient, but
if you open a lot of buffers at the same time your system may be overloaded by
the query commands. Using the user command allows you to choose what buffer to
register. Carefully choose how you register your buffers according to your
system specs and setup.


USER COMMAND                           *VectorCode-neovim-plugin-user-command*


VECTORCODE REGISTER ~

Register the current buffer for async caching. It’s possible to register the
current buffer to a different vectorcode project by passing the `project_root`
parameter:

>
    :VectorCode register project_root=path/to/another/project/
<

This is useful if you’re working on a project that is closely related to a
different project, for example a utility repository for a main library or a
documentation repository.


VECTORCODE DEREGISTER ~

Deregister the current buffer. Any running jobs will be killed, cached results
will be deleted, and no more queries will be run.


CONFIGURATION                         *VectorCode-neovim-plugin-configuration*


SETUP(OPTS?) ~

This function initialises the VectorCode client and sets up some default

>lua
    -- Default configuration
    require("vectorcode").setup({
      async_opts = {
        debounce = 10,
        events = { "BufWritePost", "InsertEnter", "BufReadPost" },
        exclude_this = true,
        n_query = 1,
        notify = false,
        query_cb = require("vectorcode.utils").make_surrounding_lines_cb(-1),
        run_on_register = false,
      },
      async_backend = "default", -- or "lsp"
      exclude_this = true,
      n_query = 1,
      notify = true,
      timeout_ms = 5000,
      on_setup = {
        update = false, -- set to true to enable update when `setup` is called.
      }
    })
<

The following are the available options for the parameter of this function: -
`n_query`: number of retrieved documents. A large number gives a higher chance
of including the right file, but with the risk of saturating the context window
and getting truncated. Default: `1`; - `notify`: whether to show notifications
when a query is completed. Default: `true`; - `timeout_ms`: timeout in
milliseconds for the query operation. Applies to synchronous API only. Default:
`5000` (5 seconds); - `exclude_this`: whether to exclude the file you’re
editing. Setting this to `false` may lead to an outdated version of the current
file being sent to the LLM as the prompt, and can lead to generations with
outdated information; - `async_opts`: default options used when registering
buffers. See |VectorCode-`register_buffer(bufnr?,-opts?)`| for details; -
`async_backend`: the async backend to use, currently either `"default"` or
`"lsp"`. Default: `"default"`; - `on_setup`: some actions that can be
registered to run when `setup` is called. Currently only supports `update`,
which calls `vectorcode update` CLI command if
|VectorCode-`vectorcode-check-config`| prints a valid path.

You may notice that a lot of options in `async_opts` are the same as the other
options in the top-level of the main option table. This is because the
top-level options are designated for the |VectorCode-synchronous-api| and the
ones in `async_opts` is for the |VectorCode-cached-asynchronous-api|. The
`async_opts` will reuse the synchronous API options if not explicitly
configured.


API USAGE                                 *VectorCode-neovim-plugin-api-usage*

This plugin provides 2 sets of APIs that provides similar functionalities. The
synchronous APIs provide more up-to-date retrieval results at the cost of
blocking the main neovim UI, while the async APIs use a caching mechanism to
provide asynchronous retrieval results almost instantaneously, but the result
may be slightly out-of-date. For some tasks like chat, the main UI being
blocked/frozen doesn’t hurt much because you spend the time waiting for
response anyway, and you can use the synchronous API in this case. For other
tasks like completion, the async API will minimise the interruption to your
workflow.


SYNCHRONOUS API ~


QUERY(QUERY_MESSAGE, OPTS?, CALLBACK?)

This function queries VectorCode and returns an array of results.

>lua
    require("vectorcode").query("some query message", {
        n_query = 5,
    })
<

- `query_message`: string or a list of strings, the query messages;
- `opts`: The following are the available options for this function (see |VectorCode-`setup(opts?)`| for details):

>lua
    {
        exclude_this = true,
        n_query = 1,
        notify = true,
        timeout_ms = 5000,
    }
<

- `callback`: a callback function that takes the result of the retrieval as the
    only parameter. If this is set, the `query` function will be non-blocking and
    runs in an async manner. In this case, it doesn’t return any value and
    retrieval results can only be accessed by this callback function.

The return value of this function is an array of results in the format of
`{path="path/to/your/code.lua", document="document content"}`.

For example, in cmp-ai <https://github.com/tzachar/cmp-ai>, you can add the
path/document content to the prompt like this:

>lua
    prompt = function(prefix, suffix)
        local retrieval_results = require("vectorcode").query("some query message", {
            n_query = 5,
        })
        for _, source in pairs(retrieval_results) do
            -- This works for qwen2.5-coder.
            file_context = file_context
                .. "<|file_sep|>"
                .. source.path
                .. "\n"
                .. source.document
                .. "\n"
        end
        return file_context
            .. "<|fim_prefix|>" 
            .. prefix 
            .. "<|fim_suffix|>" 
            .. suffix 
            .. "<|fim_middle|>"
    end
<


CHECK(CHECK_ITEM?)

This function checks if VectorCode has been configured properly for your
project. See the CLI manual for details <./cli.md>.

>lua
    require("vectorcode").check()
<

The following are the available options for this function: - `check_item`: Only
supports `"config"` at the moment. Checks if a project-local config is present.
Return value: `true` if passed, `false` if failed.

This involves the `check` command of the CLI that checks the status of the
VectorCode project setup. Use this as a pre-condition of any subsequent use of
other VectorCode APIs that may be more expensive (if this fails, VectorCode
hasn’t been properly set up for the project, and you should not use
VectorCode APIs).

The use of this API is entirely optional. You can totally ignore this and call
`query` anyway, but if `check` fails, you might be spending the waiting time
for nothing.


UPDATE(PROJECT_ROOT?)

This function calls `vectorcode update` at the current working directory.
`--project_root` will be added if the `project_root` parameter is not `nil`.
This runs async and doesn’t block the main UI.

>lua
    require("vectorcode").update()
<


CACHED ASYNCHRONOUS API ~

The async cache mechanism helps mitigate the issue where the `query` API may
take too long and block the main thread. The following are the functions
available through the `require("vectorcode.cacher")` module.

From 0.4.0, the async cache module came with 2 backends that exposes the same
interface:

1. The `default` backend which works exactly like the original implementation
used in previous versions;
2. The `lsp` based backend, which make use of the experimental `vectorcode-server`
implemented in version 0.4.0.

  -------------------------------------------------------------------------------
  Features   default                         lsp
  ---------- ------------------------------- ------------------------------------
  Pros       Fully backward compatible with  Less IO overhead for
             minimal extra config required   loading/unloading embedding models;
                                             Progress reports.

  Cons       Heavy IO overhead because the   Requires vectorcode-server; Only
             embedding model and database    works if you’re using a standalone
             client need to be initialised   ChromaDB server; May contain bugs
             for every query.                because it’s new.
  -------------------------------------------------------------------------------
You may choose which backend to use by setting the |VectorCode-`setup`| option
`async_backend`, and acquire the corresponding backend by the following API:

>lua
    local cacher_backend = require("vectorcode.config").get_cacher_backend()
<

and you can use `cacher_backend` wherever you used to use
`require("vectorcode.cacher")`. For example,
`require("vectorcode.cacher").query_from_cache(0)` becomes
`require("vectorcode.config").get_cacher_backend().query_from_cache(0)`. In the
remaining section of this documentation, I’ll use `cacher_backend` to
represent either of the backends. Unless otherwise noticed, all the
asynchronous APIs work for both backends.


CACHER_BACKEND.REGISTER_BUFFER(BUFNR?, OPTS?)

This function registers a buffer to be cached by VectorCode.

>lua
    cacher_backend.register_buffer(0, {
        n_query = 1,
    })
<

The following are the available options for this function: - `bufnr`: buffer
number. Default: current buffer; - `opts`: accepts a lua table with the
following keys: - `project_root`: a string of the path that overrides the
detected project root. Default: `nil`. This is mostly intended to use with the
|VectorCode-user-command|, and you probably should not use this directly in
your config. **If you’re using the LSP backend and did not specify this
value, it will be automatically detected based on .vectorcode or .git. If this
fails, LSP backend will not work**; - `exclude_this`: whether to exclude the
file you’re editing. Default: `true`; - `n_query`: number of retrieved
documents. Default: `1`; - `debounce`: debounce time in milliseconds. Default:
`10`; - `notify`: whether to show notifications when a query is completed.
Default: `false`; - `query_cb`: a callback function that accepts the buffer ID
and returns the query message(s). Default:
`require("vectorcode.utils").make_surrounding_lines_cb(-1)`; - `events`: list
of autocommand events that triggers the query. Default: `{"BufWritePost",
"InsertEnter", "BufReadPost"}`; - `run_on_register`: whether to run the query
when the buffer is registered. Default: `false`; - `single_job`: boolean. If
this is set to `true`, there will only be one running job for each buffer, and
when a new job is triggered, the last-running job will be cancelled. Default:
`false`.


CACHER_BACKEND.QUERY_FROM_CACHE(BUFNR?)

This function queries VectorCode from cache.

>lua
    local query_results = cacher_backend.query_from_cache(0, {notify=false})
<

The following are the available options for this function: - `bufnr`: buffer
number. Default: current buffer; - `opts`: accepts a lua table with the
following keys: - `notify`: boolean, whether to show notifications when a query
is completed. Default: `false`;

Return value: an array of results. Each item of the array is in the format of
`{path="path/to/your/code.lua", document="document content"}`.


CACHER_BACKEND.ASYNC_CHECK(CHECK_ITEM?, ON_SUCCESS?, ON_FAILURE?)

This function checks if VectorCode has been configured properly for your
project.

>lua
    cacher_backend.async_check(
        "config", 
        do_something(), -- on success
        do_something_else()  -- on failure
    )
<

The following are the available options for this function: - `check_item`: any
check that works with `vectorcode check` command. If not set, it defaults to
`"config"`; - `on_success`: a callback function that is called when the check
passes; - `on_failure`: a callback function that is called when the check
fails.


CACHER_BACKEND.BUF_IS_REGISTERED(BUFNR?)

This function checks if a buffer has been registered with VectorCode.

The following are the available options for this function: - `bufnr`: buffer
number. Default: current buffer. Return value: `true` if registered, `false`
otherwise.


CACHER_BACKEND.BUF_IS_ENABLED(BUFNR?)

This function checks if a buffer has been enabled with VectorCode. It is
slightly different from `buf_is_registered`, because it does not guarantee
VectorCode is actively caching the content of the buffer. It is the same as
`buf_is_registered && not is_paused`.

The following are the available options for this function: - `bufnr`: buffer
number. Default: current buffer. Return value: `true` if enabled, `false`
otherwise.


CACHER_BACKEND.BUF_JOB_COUNT(BUFNR?)

Returns the number of running jobs in the background.


CACHER_BACKEND.MAKE_PROMPT_COMPONENT(BUFNR?, COMPONENT_CB?)

Compile the retrieval results into a string. Parameters: - `bufnr`: buffer
number. Default: current buffer; - `component_cb`: a callback function that
formats each retrieval result, so that you can customise the control token,
etc. for the component. The default is the following:

>lua
    function(result)
        return "<|file_sep|>" .. result.path .. "\n" .. result.document
    end
<

`make_prompt_component` returns a table with 2 keys: - `count`: number of
retrieved documents; - `content`: The retrieval results concatenated together
into a string. Each result is formatted by `component_cb`.


INTEGRATIONS                           *VectorCode-neovim-plugin-integrations*

The wiki <https://github.com/Davidyz/VectorCode/wiki/Neovim-Integrations>
contains instructions and config examples to integrate VectorCode with the
following plugins:

- milanglacier/minuet-ai.nvim <https://github.com/milanglacier/minuet-ai.nvim>;
- olimorris/codecompanion.nvim <https://github.com/olimorris/codecompanion.nvim>;
- nvim-lualine/lualine.nvim <https://github.com/nvim-lualine/lualine.nvim>;
- CopilotC-Nvim/CopilotChat.nvim <https://github.com/CopilotC-Nvim/CopilotChat.nvim>.

Generated by panvimdoc <https://github.com/kdheepak/panvimdoc>

vim:tw=78:ts=8:noet:ft=help:norl:
